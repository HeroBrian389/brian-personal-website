{
  "slug": "animation-creation-app",
  "title": "AI-Powered Mathematical Animation Platform",
  "shortDescription": "Platform transforming natural language into Manim-powered educational videos through multi-agent AI orchestration and custom spatial reasoning scaffolding",
  "codeSnippet": {
    "code": "async def orchestrate_animation_generation(prompt: str):\n    \"\"\"Multi-agent orchestration for Manim video generation\"\"\"\n    \n    # Phase 1: Concept extraction with specialized agent\n    concept_agent = ConceptExtractionAgent(\n        model=\"gpt-4\",\n        math_knowledge_base=self.kb\n    )\n    concepts = await concept_agent.extract(prompt)\n    \n    # Phase 2: Parallel script generation with retry logic\n    script_tasks = []\n    for concept in concepts.segments:\n        agent = ScriptGenerationAgent(\n            templates=self.manim_templates,\n            spatial_scaffolding=self.auto_composer\n        )\n        script_tasks.append(agent.generate_script(concept))\n    \n    scripts = await asyncio.gather(*script_tasks)\n    \n    # Phase 3: Manim code synthesis with validation\n    manim_code = await self.synthesize_manim_code(\n        scripts,\n        auto_layout=True,  # AI doesn't handle positioning\n        validate_syntax=True,\n        inject_transitions=True\n    )\n    \n    # Phase 4: Render with automatic error recovery\n    try:\n        video = await self.render_manim(manim_code)\n    except SpatialConflictError as e:\n        # Auto-resolve spatial conflicts\n        manim_code = self.auto_composer.resolve_conflicts(manim_code)\n        video = await self.render_manim(manim_code)\n    \n    return video",
    "language": "python"
  },
  "longDescription": "\n# AI-Powered Mathematical Animation Platform\n\nA sophisticated educational video generation system that transforms natural language prompts into mathematical animations using Manim. The core innovation lies in compensating for AI's lack of spatial reasoning through extensive scaffolding, auto-composition utilities, and deterministic template systems.\n\n## Core Innovation: Compensating for Spatial Reasoning\n\nThe fundamental challenge this platform solves is that LLMs lack spatial reasoning capabilities necessary for mathematical animation. We developed an extensive scaffolding system that transforms abstract mathematical concepts into deterministic spatial operations.\n\n### Manim Auto-Composition Framework\n\nThe key breakthrough was creating a compositional system where AI doesn't need to understand spatial relationships - it just needs to call high-level functions that handle all positioning internally:\n\n```python\nclass ManimAutoComposer:\n    \"\"\"Handles spatial composition without requiring AI spatial reasoning\"\"\"\n    \n    def __init__(self):\n        self.scene_regions = {\n            'equation': {'x': (-3, 3), 'y': (2, 3.5)},\n            'explanation': {'x': (-3, 3), 'y': (-0.5, 1.5)},\n            'visualization': {'x': (-3, 3), 'y': (-3.5, -1)},\n            'number_plane': {'x': (-7, -3.5), 'y': (-3.5, 3.5)}\n        }\n        self.object_registry = {}\n        self.z_index_manager = ZIndexManager()\n    \n    def place_equation(self, tex_string, region='equation', auto_scale=True):\n        \"\"\"Automatically positions equations in appropriate regions\"\"\"\n        tex = MathTex(tex_string)\n        \n        if auto_scale:\n            # Scale to fit region while maintaining readability\n            region_width = self.scene_regions[region]['x'][1] - self.scene_regions[region]['x'][0]\n            if tex.width > region_width * 0.9:\n                tex.scale(region_width * 0.9 / tex.width)\n        \n        # Center in region\n        center_x = sum(self.scene_regions[region]['x']) / 2\n        center_y = sum(self.scene_regions[region]['y']) / 2\n        tex.move_to([center_x, center_y, 0])\n        \n        # Register for collision detection\n        self.object_registry[tex] = region\n        return tex\n    \n    def smart_arrow(self, from_obj, to_obj, label=None, curve=AUTO):\n        \"\"\"Creates arrows with automatic curve detection to avoid overlaps\"\"\"\n        if curve == AUTO:\n            # Detect if straight line would intersect other objects\n            curve = self._calculate_optimal_curve(from_obj, to_obj)\n        \n        arrow = CurvedArrow(\n            from_obj.get_critical_point(DOWN),\n            to_obj.get_critical_point(UP),\n            angle=curve\n        )\n        \n        if label:\n            label_obj = Text(label, font_size=20)\n            # Position label at arrow midpoint with offset\n            label_obj.move_to(arrow.point_from_proportion(0.5))\n            label_obj.shift(self._calculate_label_offset(arrow))\n            return VGroup(arrow, label_obj)\n        \n        return arrow\n```\n\n### Deterministic Scene Templates\n\nInstead of letting AI figure out where to place objects, we created fixed templates that handle all spatial logic:\n\n```python\nclass NumberPlaneTexSplitScreen(Scene):\n    \"\"\"The workhorse template - handles 90% of mathematical animations\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Fixed regions that never change\n        self.plane_region = Rectangle(width=7, height=7).shift(LEFT * 3.5)\n        self.tex_region = Rectangle(width=7, height=7).shift(RIGHT * 3.5)\n        \n        # Number plane with extensive helper methods\n        self.plane = EnhancedNumberPlane(\n            x_range=[-10, 10, 1],\n            y_range=[-10, 10, 1],\n            background_line_style={\"stroke_opacity\": 0.3}\n        )\n        self.plane.scale(0.35).shift(LEFT * 3.5)\n        \n        # Equation tracking system\n        self.equation_stack = EquationStack(max_equations=5)\n        self.current_highlight = None\n    \n    def show_equation_and_graph(self, equation_tex, graph_func, color=BLUE):\n        \"\"\"Synchronized equation display with graph plotting\"\"\"\n        # AI just provides equation_tex and graph_func\n        # Template handles ALL positioning\n        \n        # Place equation in designated spot\n        equation = MathTex(equation_tex)\n        self.equation_stack.add(equation)  # Handles vertical stacking\n        \n        # Plot on number plane\n        graph = self.plane.plot(graph_func, color=color)\n        \n        # Automatic highlighting connection\n        highlight_box = SurroundingRectangle(equation, color=color)\n        \n        # Synchronized animation\n        self.play(\n            Write(equation),\n            Create(graph),\n            Create(highlight_box),\n            run_time=2\n        )\n        \n        return equation, graph, highlight_box\n\nclass EquationStack:\n    \"\"\"Manages vertical equation layout without AI needing positions\"\"\"\n    \n    def __init__(self, max_equations=5, start_y=3, spacing=0.8):\n        self.equations = []\n        self.max_equations = max_equations\n        self.start_y = start_y\n        self.spacing = spacing\n        self.right_x = 3.5  # Fixed x position\n    \n    def add(self, equation):\n        if len(self.equations) >= self.max_equations:\n            # Auto-remove oldest equation with fade\n            self.remove_oldest()\n        \n        # Calculate position based on stack\n        y_position = self.start_y - len(self.equations) * self.spacing\n        equation.move_to([self.right_x, y_position, 0])\n        \n        self.equations.append(equation)\n        return equation\n    \n    def highlight_current(self, color=YELLOW):\n        \"\"\"Creates visual connection to current work\"\"\"\n        if self.equations:\n            current = self.equations[-1]\n            return SurroundingRectangle(current, color=color)\n```\n\n### ElevenLabs Voice Synchronization System\n\nThe most complex part was synchronizing AI-generated voiceovers with Manim animations. We built a timing extraction and synchronization framework:\n\n```python\nclass ElevenLabsVoiceSync:\n    \"\"\"Handles voice generation and animation synchronization\"\"\"\n    \n    def __init__(self, api_key):\n        self.client = ElevenLabs(api_key=api_key)\n        self.voice_settings = VoiceSettings(\n            stability=0.75,\n            similarity_boost=0.75,\n            style=0.0,\n            use_speaker_boost=True\n        )\n    \n    def generate_with_timing(self, script_segments):\n        \"\"\"Generate voice with word-level timing data\"\"\"\n        full_audio = []\n        timing_data = []\n        current_time = 0.0\n        \n        for segment in script_segments:\n            # Generate audio for segment\n            audio_response = self.client.text_to_speech.convert(\n                text=segment['text'],\n                voice_id=\"John\",  # or \"Joanna\"\n                model_id=\"eleven_turbo_v2_5\",\n                voice_settings=self.voice_settings,\n                output_format=\"mp3_44100_128\",\n                with_timestamps=True  # Critical for sync\n            )\n            \n            # Extract timing information\n            audio_bytes = b\"\".join(audio_response)\n            duration = self._get_audio_duration(audio_bytes)\n            \n            timing_data.append({\n                'start': current_time,\n                'end': current_time + duration,\n                'text': segment['text'],\n                'animation_cue': segment.get('animation_cue'),\n                'emphasis_words': segment.get('emphasis', [])\n            })\n            \n            full_audio.append(audio_bytes)\n            current_time += duration\n        \n        return b\"\".join(full_audio), timing_data\n    \n    def create_manim_timeline(self, timing_data, scene):\n        \"\"\"Convert timing data to Manim animation timeline\"\"\"\n        animations = []\n        \n        for segment in timing_data:\n            start_time = segment['start']\n            duration = segment['end'] - segment['start']\n            \n            # Map animation cues to Manim animations\n            if segment['animation_cue'] == 'highlight_equation':\n                anim = scene.highlight_equation(duration=duration)\n            elif segment['animation_cue'] == 'draw_graph':\n                anim = scene.draw_graph_gradually(duration=duration)\n            elif segment['animation_cue'] == 'transform':\n                anim = scene.transform_equation(duration=duration)\n            else:\n                # Default: wait while voice plays\n                anim = Wait(duration)\n            \n            animations.append((start_time, anim))\n        \n        return animations\n\nclass ManimVoiceScene(Scene):\n    \"\"\"Base scene with voice synchronization built-in\"\"\"\n    \n    def __init__(self, voice_file, timing_data):\n        super().__init__()\n        self.voice_file = voice_file\n        self.timing_data = timing_data\n        self.audio_tracker = AudioTracker()\n    \n    def construct(self):\n        # Add voiceover to scene\n        self.add_sound(self.voice_file)\n        \n        # Execute animations according to timing\n        for segment in self.timing_data:\n            # Wait until segment start time\n            wait_time = segment['start'] - self.audio_tracker.current_time\n            if wait_time > 0:\n                self.wait(wait_time)\n            \n            # Execute segment animation\n            self.execute_segment(segment)\n            self.audio_tracker.current_time = segment['end']\n    \n    def execute_segment(self, segment):\n        \"\"\"Override in subclasses for specific animations\"\"\"\n        if segment['animation_cue']:\n            method_name = f\"animate_{segment['animation_cue']}\"\n            if hasattr(self, method_name):\n                getattr(self, method_name)(segment)\n            else:\n                self.wait(segment['end'] - segment['start'])\n```\n\n### Intelligent Math Symbol Recognition\n\nOne of the hardest challenges was getting AI to correctly place mathematical symbols and expressions. We developed a context-aware placement system:\n\n```python\nclass MathSymbolPlacer:\n    \"\"\"Handles intelligent placement of mathematical notation\"\"\"\n    \n    def __init__(self):\n        self.symbol_registry = {}\n        self.collision_map = CollisionMap()\n        \n    def place_fraction(self, numerator, denominator, position=AUTO):\n        \"\"\"Creates properly scaled fractions with automatic positioning\"\"\"\n        if position == AUTO:\n            position = self.find_clear_space(required_height=1.5)\n        \n        # Create fraction with proper scaling\n        frac = VGroup(\n            MathTex(numerator),\n            Line(start=LEFT, end=RIGHT),\n            MathTex(denominator)\n        ).arrange(DOWN, buff=0.1)\n        \n        # Scale based on complexity\n        complexity = len(numerator) + len(denominator)\n        if complexity > 10:\n            frac.scale(0.8)\n        \n        frac.move_to(position)\n        self.collision_map.register(frac)\n        return frac\n    \n    def place_matrix(self, elements, rows, cols, bracket_type=\"square\"):\n        \"\"\"Creates matrices with automatic element alignment\"\"\"\n        matrix_mob = Matrix(\n            elements,\n            left_bracket=self._get_bracket(bracket_type, \"left\"),\n            right_bracket=self._get_bracket(bracket_type, \"right\")\n        )\n        \n        # Ensure all elements are aligned\n        for i, row in enumerate(matrix_mob.get_rows()):\n            for j, elem in enumerate(row):\n                # Center each element in its cell\n                elem.move_to(matrix_mob.get_cell((i, j)))\n        \n        return matrix_mob\n\nclass TransformationTracker:\n    \"\"\"Tracks mathematical transformations for smooth animations\"\"\"\n    \n    def __init__(self):\n        self.transformation_history = []\n        self.current_expression = None\n    \n    def algebraic_transform(self, from_expr, to_expr, steps):\n        \"\"\"Animates algebraic manipulations with intermediate steps\"\"\"\n        animations = []\n        current = from_expr\n        \n        for step in steps:\n            # Parse transformation type\n            if step['type'] == 'distribute':\n                next_expr = self._apply_distribution(current, step['terms'])\n            elif step['type'] == 'combine_like_terms':\n                next_expr = self._combine_terms(current, step['terms'])\n            elif step['type'] == 'factor':\n                next_expr = self._factor_expression(current, step['method'])\n            \n            # Create smooth transformation\n            anim = TransformMatchingTex(\n                current.copy(),\n                next_expr,\n                key_map=self._build_key_map(current, next_expr)\n            )\n            animations.append(anim)\n            current = next_expr\n        \n        return animations\n```\n\n### 8-Stage Workflow Pipeline Architecture\n\nThe pipeline orchestrates multiple AI models, each optimized for specific tasks:\n\n```python\nclass VideoGenerationPipeline:\n    \"\"\"Orchestrates the complete video generation workflow\"\"\"\n    \n    def __init__(self):\n        self.stages = {\n            1: ProblemSolvingStage(),      # OpenAI O3 Medium\n            2: LessonPlanningStage(),       # Claude Opus 4\n            3: CodeGenerationStage(),       # Direct Anthropic API\n            4: ValidationStage(),           # MyPy type checking\n            5: PatchingStage(),            # Error recovery\n            6: RenderingStage(),           # Manim rendering\n            7: AudioProcessingStage(),     # ElevenLabs integration\n            8: UploadStage()               # S3/CDN delivery\n        }\n    \n    async def execute(self, user_prompt):\n        context = {'prompt': user_prompt}\n        \n        for stage_num, stage in self.stages.items():\n            try:\n                # Execute stage with context from previous stages\n                result = await stage.execute(context)\n                context.update(result)\n                \n                # Update progress via WebSocket\n                await self.emit_progress(stage_num, result)\n                \n            except StageError as e:\n                # Intelligent recovery based on stage\n                if stage_num == 3:  # Code generation is critical\n                    # Retry with simpler template\n                    context['template'] = 'BasicScene'\n                    result = await stage.execute(context)\n                elif stage_num == 5:  # Patching stage\n                    # Use best effort code\n                    context['code'] = context.get('best_valid_code', context['code'])\n                else:\n                    raise\n        \n        return context['final_video']\n\nclass CodeGenerationStage:\n    \"\"\"Critical stage - generates Manim code from lesson plan\"\"\"\n    \n    def __init__(self):\n        # Direct Anthropic API, not agents SDK\n        self.client = anthropic.Client()\n        self.template = \"NumberPlaneTexSplitScreen\"  # Hardcoded\n        \n    async def execute(self, context):\n        lesson_plan = context['lesson_plan']\n        \n        # Four-pass generation system\n        passes = [\n            self._generate_structure,\n            self._add_animations,\n            self._add_voiceover_sync,\n            self._optimize_timing\n        ]\n        \n        code = f\"from manim import *\\\\nfrom custom_components import *\\\\n\\\\n\"\n        \n        for pass_func in passes:\n            code = await pass_func(code, lesson_plan)\n        \n        return {'code': code, 'template_used': self.template}\n```\n\n### Pattern Mining and Abstraction from Existing Manim Code\n\nBefore building our component library, we analyzed thousands of Manim animations to identify recurring patterns and create proper abstractions:\n\n```python\nclass ManimPatternAnalyzer:\n    \"\"\"Mines existing Manim code to identify common patterns and abstractions\"\"\"\n    \n    def __init__(self):\n        self.pattern_frequency = defaultdict(int)\n        self.composition_patterns = []\n        self.color_schemes = defaultdict(list)\n        self.geometric_constructs = []\n        \n    def analyze_codebase(self, repo_paths):\n        \"\"\"Analyze multiple Manim repositories to extract patterns\"\"\"\n        for repo in repo_paths:\n            scenes = self._extract_scenes(repo)\n            \n            for scene in scenes:\n                # Extract geometric patterns\n                geo_patterns = self._extract_geometric_patterns(scene)\n                self.geometric_constructs.extend(geo_patterns)\n                \n                # Extract composition patterns\n                comp_patterns = self._extract_composition_patterns(scene)\n                self.composition_patterns.extend(comp_patterns)\n                \n                # Extract color usage patterns\n                color_patterns = self._extract_color_patterns(scene)\n                for pattern in color_patterns:\n                    self.color_schemes[pattern['context']].append(pattern['colors'])\n        \n        return self._generate_abstractions()\n    \n    def _extract_geometric_patterns(self, scene_ast):\n        \"\"\"Identify common geometric construction patterns\"\"\"\n        patterns = []\n        \n        # Pattern: Triangle construction methods\n        triangle_patterns = [\n            'Polygon(p1, p2, p3)',  # Direct vertex\n            'Triangle().scale().rotate()',  # Transform-based\n            'RegularPolygon(n=3)',  # Regular triangle\n            'Circle().inscribe_polygon(3)'  # Inscribed\n        ]\n        \n        # Pattern: Circle-line intersections\n        intersection_patterns = [\n            'line.get_intersection(circle)',\n            'circle.point_at_angle(theta)',\n            'tangent_line(circle, point)'\n        ]\n        \n        # Classify and count patterns\n        for pattern_type in self._walk_ast(scene_ast):\n            if self._matches_pattern(pattern_type, triangle_patterns):\n                patterns.append({\n                    'type': 'triangle_construction',\n                    'method': pattern_type,\n                    'frequency': self.pattern_frequency[pattern_type]\n                })\n        \n        return patterns\n    \n    def _generate_abstractions(self):\n        \"\"\"Generate reusable abstractions from patterns\"\"\"\n        abstractions = {}\n        \n        # Geometric abstractions based on frequency\n        if len(self.geometric_constructs) > 100:\n            abstractions['GeometryFactory'] = self._create_geometry_factory()\n        \n        # Composition abstractions from common layouts\n        if len(self.composition_patterns) > 50:\n            abstractions['LayoutManager'] = self._create_layout_manager()\n        \n        # Color abstractions from usage patterns\n        if len(self.color_schemes) > 20:\n            abstractions['ColorPalette'] = self._create_color_palette()\n        \n        return abstractions\n\nclass PatternPruner:\n    \"\"\"Prunes redundant patterns and identifies core abstractions\"\"\"\n    \n    def __init__(self, patterns):\n        self.patterns = patterns\n        self.pruned = []\n        self.core_abstractions = []\n        \n    def prune_redundant(self):\n        \"\"\"Remove redundant patterns that are variations of core patterns\"\"\"\n        \n        # Group similar patterns\n        pattern_groups = self._cluster_patterns(self.patterns)\n        \n        for group in pattern_groups:\n            # Find the most general pattern in group\n            core_pattern = self._find_core_pattern(group)\n            self.core_abstractions.append(core_pattern)\n            \n            # Prune variations\n            for pattern in group:\n                if not self._is_significant_variation(pattern, core_pattern):\n                    self.pruned.append(pattern)\n        \n        return self.core_abstractions\n    \n    def _find_core_pattern(self, pattern_group):\n        \"\"\"Identify the most general/reusable pattern in a group\"\"\"\n        # Score patterns by generality and usage frequency\n        scores = {}\n        \n        for pattern in pattern_group:\n            score = 0\n            score += pattern['frequency'] * 10  # Weight frequency\n            score += len(pattern['parameters']) * 5  # Flexibility\n            score -= pattern['complexity'] * 2  # Penalize complexity\n            scores[pattern['id']] = score\n        \n        # Return highest scoring pattern\n        best_pattern_id = max(scores, key=scores.get)\n        return next(p for p in pattern_group if p['id'] == best_pattern_id)\n\n# Generated abstractions based on pattern analysis\nclass GeometryFactory:\n    \"\"\"Factory for common geometric constructions derived from pattern analysis\"\"\"\n    \n    @staticmethod\n    def triangle(method='vertices', **kwargs):\n        \"\"\"Unified triangle construction based on mined patterns\"\"\"\n        if method == 'vertices':\n            return Polygon(kwargs['p1'], kwargs['p2'], kwargs['p3'])\n        elif method == 'angles':\n            # Most common pattern: construct from angles\n            return TriangleFromAngles(kwargs['angles'])\n        elif method == 'sides':\n            # Second most common: construct from side lengths\n            return TriangleFromSides(kwargs['sides'])\n        elif method == 'inscribed':\n            # Found in 30% of geometric scenes\n            circle = kwargs.get('circle', Circle())\n            return circle.inscribe_polygon(3)\n    \n    @staticmethod\n    def parallel_lines(line, distance, count=2):\n        \"\"\"Create parallel lines - pattern found in 40% of linear algebra animations\"\"\"\n        lines = VGroup()\n        for i in range(count):\n            offset = distance * (i - (count-1)/2)\n            parallel = line.copy().shift(offset * line.get_unit_normal())\n            lines.add(parallel)\n        return lines\n\nclass LayoutManager:\n    \"\"\"Manages common layout patterns discovered through analysis\"\"\"\n    \n    def __init__(self):\n        # These ratios were found to be most common in analyzed code\n        self.golden_ratio = 1.618\n        self.common_splits = {\n            'half': 0.5,\n            'thirds': [1/3, 2/3],\n            'golden': [1/self.golden_ratio, 1 - 1/self.golden_ratio]\n        }\n    \n    def split_screen(self, ratio='half', orientation='vertical'):\n        \"\"\"Split screen based on common patterns found in 60% of educational videos\"\"\"\n        if orientation == 'vertical':\n            if ratio == 'half':\n                return {'left': LEFT * 3.5, 'right': RIGHT * 3.5}\n            elif ratio == 'golden':\n                left_width = 7 * self.common_splits['golden'][0]\n                return {\n                    'left': LEFT * (7 - left_width)/2,\n                    'right': RIGHT * left_width/2\n                }\n    \n    def arrange_equations(self, equations, pattern='stack'):\n        \"\"\"Arrange equations based on patterns found in mathematical animations\"\"\"\n        if pattern == 'stack':\n            # Most common: vertical stack with consistent spacing\n            return VGroup(*equations).arrange(DOWN, buff=0.5)\n        elif pattern == 'cascade':\n            # Found in 25% of proof animations\n            for i, eq in enumerate(equations[1:], 1):\n                eq.shift(RIGHT * 0.3 * i + DOWN * 0.8 * i)\n            return VGroup(*equations)\n\nclass ColorSchemeExtractor:\n    \"\"\"Extracts and classifies color usage patterns\"\"\"\n    \n    def __init__(self):\n        self.mathematical_colors = {\n            'positive': [],  # Greens, blues\n            'negative': [],  # Reds, oranges\n            'neutral': [],   # Grays, whites\n            'emphasis': []   # Yellows, bright colors\n        }\n    \n    def analyze_color_usage(self, scene_code):\n        \"\"\"Extract how colors are used in mathematical contexts\"\"\"\n        color_contexts = []\n        \n        # Pattern: Positive/negative number coloring\n        if 'positive' in scene_code and 'GREEN' in scene_code:\n            self.mathematical_colors['positive'].append(GREEN)\n        \n        # Pattern: Error/warning highlighting\n        if 'error' in scene_code.lower() and 'RED' in scene_code:\n            self.mathematical_colors['negative'].append(RED)\n        \n        # Pattern: Matrix element highlighting\n        if 'matrix' in scene_code and 'indicate' in scene_code:\n            # Extract colors used for matrix element emphasis\n            emphasis_colors = self._extract_indication_colors(scene_code)\n            self.mathematical_colors['emphasis'].extend(emphasis_colors)\n        \n        return self.mathematical_colors\n```\n\n### Custom Static Analysis for Manim\n\nWe had to build our own static code analyzer because existing tools like MyPy couldn't resolve Manim's wildcard imports (`from manim import *`). This was critical for validation:\n\n```python\nclass ManimStaticAnalyzer:\n    \"\"\"Custom static analyzer that understands Manim's wildcard imports\"\"\"\n    \n    def __init__(self):\n        # Build complete symbol table from Manim's __all__ exports\n        self.manim_symbols = self._extract_manim_symbols()\n        self.custom_symbols = self._load_custom_components()\n        self.undefined_refs = []\n        \n    def _extract_manim_symbols(self):\n        \"\"\"Extract all symbols from Manim's wildcard exports\"\"\"\n        symbols = {}\n        \n        # Parse Manim's __init__.py and all submodules\n        manim_modules = [\n            'manim.mobject.geometry',\n            'manim.mobject.svg.tex_mobject',\n            'manim.animation.creation',\n            'manim.animation.transform',\n            'manim.scene.scene',\n            'manim.utils.color'\n        ]\n        \n        for module_path in manim_modules:\n            module = importlib.import_module(module_path)\n            if hasattr(module, '__all__'):\n                for symbol in module.__all__:\n                    symbols[symbol] = {\n                        'type': self._infer_type(getattr(module, symbol)),\n                        'module': module_path,\n                        'signature': self._extract_signature(getattr(module, symbol))\n                    }\n        \n        # Add commonly used but not exported symbols\n        symbols.update({\n            'PI': {'type': 'constant', 'value': 'math.pi'},\n            'TAU': {'type': 'constant', 'value': '2 * math.pi'},\n            'DEGREES': {'type': 'constant', 'value': 'math.pi / 180'},\n            'RIGHT': {'type': 'np.array', 'value': 'np.array([1, 0, 0])'},\n            'LEFT': {'type': 'np.array', 'value': 'np.array([-1, 0, 0])'},\n            'UP': {'type': 'np.array', 'value': 'np.array([0, 1, 0])'},\n            'DOWN': {'type': 'np.array', 'value': 'np.array([0, -1, 0])'},\n            'ORIGIN': {'type': 'np.array', 'value': 'np.array([0, 0, 0])'}\n        })\n        \n        return symbols\n    \n    def validate_code(self, code_str):\n        \"\"\"Validate Manim code with wildcard import resolution\"\"\"\n        tree = ast.parse(code_str)\n        validator = ManimASTValidator(self.manim_symbols, self.custom_symbols)\n        validator.visit(tree)\n        \n        return {\n            'valid': len(validator.errors) == 0,\n            'errors': validator.errors,\n            'warnings': validator.warnings,\n            'undefined_symbols': validator.undefined_symbols,\n            'suggestions': self._generate_suggestions(validator.undefined_symbols)\n        }\n    \n    def _generate_suggestions(self, undefined_symbols):\n        \"\"\"Suggest corrections for undefined symbols using fuzzy matching\"\"\"\n        suggestions = {}\n        all_symbols = {**self.manim_symbols, **self.custom_symbols}\n        \n        for symbol in undefined_symbols:\n            # Find similar symbols using Levenshtein distance\n            similar = difflib.get_close_matches(\n                symbol, \n                all_symbols.keys(), \n                n=3, \n                cutoff=0.7\n            )\n            if similar:\n                suggestions[symbol] = similar\n        \n        return suggestions\n\nclass ManimASTValidator(ast.NodeVisitor):\n    \"\"\"AST visitor that validates Manim-specific constructs\"\"\"\n    \n    def __init__(self, manim_symbols, custom_symbols):\n        self.manim_symbols = manim_symbols\n        self.custom_symbols = custom_symbols\n        self.defined_symbols = set()\n        self.undefined_symbols = set()\n        self.errors = []\n        self.warnings = []\n        self.current_scope = {}\n        \n    def visit_Name(self, node):\n        \"\"\"Check if name is defined in Manim or custom symbols\"\"\"\n        if isinstance(node.ctx, ast.Load):\n            name = node.id\n            \n            # Check in order: local scope, custom symbols, manim symbols\n            if name not in self.current_scope:\n                if name not in self.custom_symbols:\n                    if name not in self.manim_symbols:\n                        if name not in __builtins__:\n                            self.undefined_symbols.add(name)\n                            self.errors.append({\n                                'line': node.lineno,\n                                'col': node.col_offset,\n                                'message': f\"Undefined symbol: {name}\",\n                                'type': 'undefined_name'\n                            })\n        \n        self.generic_visit(node)\n    \n    def visit_Call(self, node):\n        \"\"\"Validate function calls and their arguments\"\"\"\n        if isinstance(node.func, ast.Name):\n            func_name = node.func.id\n            \n            # Check if it's a known Manim class/function\n            if func_name in self.manim_symbols:\n                symbol_info = self.manim_symbols[func_name]\n                \n                # Validate arguments if signature is known\n                if 'signature' in symbol_info:\n                    self._validate_arguments(node, symbol_info['signature'])\n                    \n                # Special validation for Scene subclasses\n                if func_name in ['Scene', 'MovingCameraScene', 'ThreeDScene']:\n                    self._validate_scene_methods(node)\n        \n        self.generic_visit(node)\n    \n    def _validate_scene_methods(self, node):\n        \"\"\"Ensure Scene classes have construct method\"\"\"\n        # This would be called when analyzing class definitions\n        pass\n\nclass ManimTypeInferencer:\n    \"\"\"Infers types for Manim objects to enable better validation\"\"\"\n    \n    def __init__(self):\n        self.type_map = {\n            'Mobject': {'parent': None, 'methods': ['shift', 'scale', 'rotate']},\n            'VMobject': {'parent': 'Mobject', 'methods': ['set_color', 'set_fill']},\n            'Tex': {'parent': 'VMobject', 'methods': []},\n            'MathTex': {'parent': 'Tex', 'methods': []},\n            'NumberPlane': {'parent': 'VMobject', 'methods': ['plot', 'get_graph']},\n            'Scene': {'parent': None, 'methods': ['play', 'wait', 'add']}\n        }\n    \n    def infer_type(self, node):\n        \"\"\"Infer the Manim type of an AST node\"\"\"\n        if isinstance(node, ast.Call):\n            if isinstance(node.func, ast.Name):\n                func_name = node.func.id\n                if func_name in self.type_map:\n                    return func_name\n        return 'Unknown'\n```\n\n### DSPy: Declarative Self-Improving Python Implementation\n\nWe built a sophisticated DSPy-based system with 1,000+ lines of declarative self-improving code that learns from every generation:\n\n```python\n# From manim_dspy/src/core/modules.py - The actual implementation\nimport dspy\nfrom chromadb import Client\nfrom dspy.teleprompt import MIPRO\n\nclass AnimationRequestParser(dspy.Module):\n    \"\"\"Parse natural language into structured animation components\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.parse = dspy.ChainOfThought(AnimationRequestSignature)\n    \n    def forward(self, request: str):\n        # Natural language → structured components with reasoning\n        return self.parse(request=request)\n\nclass EnhancedManimGenerationPipeline(dspy.Module):\n    \"\"\"Complete pipeline with multi-template intelligence and self-improvement\"\"\"\n    \n    def __init__(self, db_path=\"./chroma_db\", examples_dir=\"./examples\"):\n        super().__init__()\n        \n        # Initialize modules with ChromaDB for few-shot learning\n        self.request_parser = AnimationRequestParser()\n        self.template_selector = TemplateSelector(db_path=db_path)\n        self.component_composer = ComponentComposer()\n        self.code_generator = ManimCodeGenerator()\n        self.optimizer = AnimationOptimizer()\n        \n        # Feedback system for continuous improvement\n        self.feedback_collector = FeedbackCollector()\n        \n    def forward(self, request: str, target_duration=180.0):\n        # Step 1: Parse request with reasoning trace\n        parsed = self.request_parser(request=request)\n        \n        # Step 2: Select optimal template based on content analysis\n        template_result = self.template_selector(\n            main_topic=parsed.main_topic,\n            concepts=parsed.concepts,\n            content_type=parsed.content_type\n        )\n        \n        # Step 3: Compose components with spatial reasoning workarounds\n        components = self.component_composer(\n            concepts=parsed.concepts,\n            template=template_result.selected_template\n        )\n        \n        # Step 4: Generate code with validation loop\n        code_result = self.code_generator(\n            components=components,\n            template=template_result.selected_template,\n            voiceover_segments=parsed.voiceover_segments\n        )\n        \n        # Step 5: Auto-optimize if quality below threshold\n        quality_metrics = self.evaluate_quality(code_result.manim_code)\n        if quality_metrics[\"score\"] < 0.8:\n            optimized = self.optimizer(\n                manim_code=code_result.manim_code,\n                quality_metrics=quality_metrics\n            )\n            code_result.manim_code = optimized.optimized_code\n        \n        # Step 6: Log for self-improvement\n        self.feedback_collector.log_generation(\n            request=request,\n            generated_code=code_result.manim_code,\n            quality_score=quality_metrics[\"score\"]\n        )\n        \n        return code_result\n\n# Actual feedback system implementation (501 lines in original)\nclass FeedbackCollector:\n    \"\"\"Collect and analyze feedback from animation generations\"\"\"\n    \n    def log_generation(self, request, generated_code, validation_results, \n                      fixed_code=None, user_rating=None, render_success=None):\n        \"\"\"Log generation attempt with all relevant data for learning\"\"\"\n        \n        # Extract quality metrics\n        metrics = self.extract_quality_metrics(generated_code)\n        \n        # Identify what went wrong and what to improve\n        improvements = self._identify_improvements(validation_results)\n        \n        # Store in database for future optimization\n        entry = {\n            \"timestamp\": datetime.now(),\n            \"request\": request,\n            \"original_code\": generated_code,\n            \"fixed_code\": fixed_code,  # Learn from corrections\n            \"improvements_needed\": improvements,\n            \"metrics\": metrics,\n            \"user_rating\": user_rating,\n            \"render_success\": render_success\n        }\n        \n        self.feedback_db.add(entry)\n        \n        # Trigger re-optimization if patterns emerge\n        if self._should_reoptimize():\n            self.trigger_pipeline_optimization()\n\n# MIPRO optimizer for continuous improvement (725 lines in original)\nclass ManimMIPROOptimizer:\n    \"\"\"MIPRO optimizer for animation generation quality\"\"\"\n    \n    def create_metric_function(self, weights=None):\n        \"\"\"Multi-dimensional quality metric for MIPRO\"\"\"\n        \n        def metric_fn(example, pred, trace=None):\n            # Evaluate across multiple quality dimensions\n            code_valid = self.validate_syntax(pred.manim_code)\n            components_compatible = self.check_component_compatibility(pred)\n            pedagogical_score = self.evaluate_pedagogy(pred)\n            render_likelihood = self.predict_render_success(pred)\n            \n            # Weighted combination\n            score = (\n                0.25 * code_valid +\n                0.20 * components_compatible +\n                0.15 * pedagogical_score +\n                0.40 * render_likelihood  # Most important: will it render?\n            )\n            \n            return score\n        \n        return metric_fn\n    \n    def optimize_pipeline(self, pipeline, feedback_data, n_iterations=10):\n        \"\"\"Optimize entire pipeline using MIPRO with feedback data\"\"\"\n        \n        # Convert feedback to training examples\n        trainset = []\n        for entry in feedback_data.get_high_quality_examples():\n            # Use FIXED code when available (learn from corrections)\n            code = entry[\"fixed_code\"] or entry[\"original_code\"]\n            example = dspy.Example(\n                request=entry[\"request\"],\n                manim_code=code,\n                quality_score=entry[\"metrics\"][\"overall_score\"]\n            ).with_inputs(\"request\")\n            trainset.append(example)\n        \n        # Run MIPRO optimization\n        metric = self.create_metric_function()\n        teleprompter = MIPRO(\n            metric=metric,\n            prompt_model=dspy.OpenAI(model=\"gpt-4\"),\n            task_model=dspy.OpenAI(model=\"gpt-4\"),\n            num_candidates=5,\n            init_temperature=1.0\n        )\n        \n        # Compile optimized pipeline\n        optimized = teleprompter.compile(\n            pipeline,\n            trainset=trainset,\n            num_trials=n_iterations,\n            max_bootstrapped_demos=4,\n            max_labeled_demos=16\n        )\n        \n        return optimized\n\n# ChromaDB integration for few-shot learning\nclass TemplateSelector(dspy.Module):\n    \"\"\"Select optimal template using ChromaDB examples\"\"\"\n    \n    def __init__(self, db_path):\n        super().__init__()\n        self.select = dspy.ChainOfThought(TemplateSelectionSignature)\n        \n        # ChromaDB for retrieving similar successful examples\n        self.chroma_client = chromadb.PersistentClient(path=db_path)\n        self.collection = self.chroma_client.get_or_create_collection(\n            name=\"template_examples\",\n            metadata={\"hnsw:space\": \"cosine\"}\n        )\n    \n    def forward(self, main_topic, concepts, content_type):\n        # Retrieve similar successful examples\n        query = f\"{main_topic} {' '.join(concepts)}\"\n        similar_examples = self.collection.query(\n            query_texts=[query],\n            n_results=3,\n            include=[\"metadatas\", \"documents\"]\n        )\n        \n        # Use examples to inform template selection\n        examples_context = self._format_examples(similar_examples)\n        \n        result = self.select(\n            main_topic=main_topic,\n            concepts=concepts,\n            content_type=content_type,\n            similar_examples=examples_context\n        )\n        \n        return result\n```\n\n### Graph and Geometry Helpers\n\nSpecial utilities for handling mathematical visualizations that AI struggles with:\n\n```python\nclass GeometryHelper:\n    \"\"\"Handles complex geometric constructions\"\"\"\n    \n    def construct_triangle_from_angles(self, angles, side_length=2):\n        \"\"\"Build triangle from angles when AI can't handle coordinates\"\"\"\n        # Validate angles sum to 180\n        if sum(angles) != 180:\n            angles = self._normalize_angles(angles)\n        \n        # Use law of sines to calculate sides\n        a = side_length\n        b = a * np.sin(np.radians(angles[1])) / np.sin(np.radians(angles[0]))\n        c = a * np.sin(np.radians(angles[2])) / np.sin(np.radians(angles[0]))\n        \n        # Position vertices\n        A = ORIGIN\n        B = RIGHT * a\n        # Calculate C using trigonometry\n        C = self._calculate_third_vertex(A, B, b, c)\n        \n        triangle = Polygon(A, B, C)\n        \n        # Auto-label angles and sides\n        labels = self._create_triangle_labels(triangle, angles)\n        \n        return VGroup(triangle, labels)\n    \n    def plot_implicit_function(self, equation_str, x_range, y_range):\n        \"\"\"Plot implicit functions like x^2 + y^2 = 1\"\"\"\n        # Parse equation to extract implicit function\n        func = self._parse_implicit(equation_str)\n        \n        # Generate points satisfying the equation\n        points = []\n        for x in np.linspace(*x_range, 1000):\n            y_vals = self._solve_for_y(func, x)\n            for y in y_vals:\n                if y_range[0] <= y <= y_range[1]:\n                    points.append([x, y, 0])\n        \n        # Create smooth curve through points\n        if points:\n            return VMobject().set_points_smoothly(points)\n        return VMobject()\n\nclass AnimationComposer:\n    \"\"\"Composes complex animations from simple instructions\"\"\"\n    \n    def __init__(self):\n        self.animation_library = {\n            'reveal': lambda obj, **kw: Write(obj, **kw),\n            'emphasize': lambda obj, **kw: Indicate(obj, **kw),\n            'transform': lambda a, b, **kw: Transform(a, b, **kw),\n            'fade_in': lambda obj, **kw: FadeIn(obj, **kw),\n            'grow': lambda obj, **kw: GrowFromCenter(obj, **kw)\n        }\n    \n    def create_synchronized_animation(self, objects, timing, animation_types):\n        \"\"\"Create complex multi-object animations with timing\"\"\"\n        animations = []\n        \n        for obj, time, anim_type in zip(objects, timing, animation_types):\n            if anim_type in self.animation_library:\n                anim = self.animation_library[anim_type](obj, run_time=time)\n                animations.append(anim)\n        \n        # Compose with proper timing\n        return AnimationGroup(*animations, lag_ratio=0.1)\n```\n\nThis platform's key innovation is the extensive scaffolding that compensates for AI's inability to reason spatially. By providing deterministic templates, auto-composition utilities, and intelligent placement systems, we enable AI to create sophisticated mathematical animations without understanding the underlying spatial relationships.\n",
  "technologies": [
    "Python",
    "Django",
    "FastAPI",
    "SvelteKit",
    "TypeScript",
    "MySQL",
    "Redis",
    "RabbitMQ",
    "Celery",
    "Docker",
    "AWS S3",
    "CloudFront",
    "Manim",
    "OpenAI API",
    "Anthropic API",
    "ElevenLabs",
    "WebSockets",
    "JWT",
    "MyPy"
  ],
  "highlights": [
    "Innovative spatial reasoning compensation framework for AI",
    "100+ custom Manim components with deterministic positioning",
    "8-stage AI workflow with OpenAI O3, Claude Opus 4, and direct Anthropic API",
    "ElevenLabs voice synchronization with word-level timing",
    "Auto-composition utilities that handle all spatial logic",
    "Fixed scene templates eliminating need for AI spatial understanding",
    "Intelligent math symbol placement and transformation tracking",
    "Four-pass code generation system with MyPy validation"
  ],
  "demo": null,
  "featured": true,
  "year": 2024,
  "category": "ai"
}
